# Read-the-papers

## 논문 리딩 스터디 진행 방식
- 논문의 종류 파악 → Abstract 분석 및 정리 → 논문의 의도 파악 → 1회독 후 요약 및 Reference 분석 → 의문점 및 추가 논의 → 2회독, 3회독 진행

### 1. **논문의 종류 파악**  
논문을 읽기 전, 해당 논문의 성격을 먼저 파악하여 접근 방식을 결정.  
- **아이디어 중심 논문:** 새로운 개념, 프레임워크, 또는 문제 정의를 제안하는 논문.  
- **이론 및 수식 중심 논문:** 수학적 이론, 알고리즘 분석, 또는 복잡도 연구에 초점을 둔 논문.  
- **실험 중심 논문:** 실험 결과와 분석을 통해 기존 연구를 보완하거나 새로운 방법론을 검증한 논문.  

### 2. **Abstract 분석 및 정리**  
- Abstract는 논문의 핵심 메시지가 응축된 부분으로, 논문의 전체 구조를 파악하는 데 중요한 역할을 함.  
- Abstract를 읽고 다음 질문에 답합니다:  
  - 논문의 주요 목표는 무엇인가?  
  - 제안된 방법론 또는 기여는 무엇인가?  
  - 실험 결과 및 결론은 어떤 점을 강조하고 있는가?  
- Abstract 내용을 가지고 논문에서 설명하려는것을 한두문장으로 요약합니다.  

### 3. **논문의 의도 파악**  
논문을 읽으면서 저자가 해결하려는 문제와 기여도를 이해.  
- **기존 연구 대비 나아진 점**: 이 논문이 어떤 문제를 어떻게 개선했는가?  
- **논문의 승인 이유 추정**: 왜 이 논문이 학계에서 인정받았을까?  

### 4. **1회독 후 요약 및 Reference 분석**
- **핵심 요약**: 논문의 주요 메시지와 결과를 간략히 정리.
- **Reference 분석 방법**:
  1. **Abstract와 Introduction**: 주요 Reference를 확인.
  2. **Problem Statement 섹션**: 기존 연구와의 차별성을 파악.
  3. **Methods 및 Results**: 핵심 기법 및 비교 연구 기반 Reference 확인.

### 5. **의문점 및 추가 논의**  
- 논문에서 해결되지 않은 의문점이나 부족한 부분 검색이나 논의.  
- 해당 연구의 응용 가능성과 개선 방향에 대해 논의합니다.  
  - **실제 문제에 적용 가능성은?**  
  - **방법론의 확장 또는 개선 방안은?**  


<details>
    <summary>2023 Summer</summary>
    
|Number|Title|Link|Github|
|:---:|:---:|:---:|:---:|
|1|ARviz – An Augmented Reality-enabled Visualization Platform for ROS Applications|https://arxiv.org/abs/2110.15521|[qor6](https://github.com/qor6)|
|2|Development of Fake News Model using Machine Learning through Natural Language Processing|https://arxiv.org/abs/2201.07489|[Seungwon62](https://github.com/Seungwon62)|
|3|NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion|https://arxiv.org/abs/2302.10109|[choo121600](https://github.com/choo121600)|
|4|Semantic-SAM: Segment and Recognize Anything at Any Granularity|https://arxiv.org/pdf/2307.04767v1.pdf|[rnjswn](https://github.com/rnjswn)|
|5|Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training|https://arxiv.org/abs/2208.06102|[hemham](https://github.com/hemham)|
|6|TVM: An Automated End-to-End Optimizing Compiler for Deep Learning|https://arxiv.org/abs/1802.04799|[cpprhtn](https://github.com/cpprhtn)|
|7|Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors|https://arxiv.org/abs/2306.17843|[hemham](https://github.com/hemham)|
|8|Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving|https://arxiv.org/abs/2308.01471|[rnjswn](https://github.com/rnjswn)|
|9|h2oGPT: Democratizing Large Language Models|https://arxiv.org/pdf/2306.08161v2.pdf|[Seungwon62](https://github.com/Seungwon62)|
</details>

<details open>
    <summary>2023 Winter</summary>
    
|Number|Title|Link|Name|
|:---:|:---:|:---:|:---:|
|1|HybridNets: End-to-End Perception Network|[https://arxiv.org/abs/2203.09035](https://arxiv.org/abs/2203.09035)|[권현주](https://github.com/rnjswn)|
|2|YOLOv4: Optimal Speed and Accuracy of Object Detection|[https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934)|[오승연](https://github.com/syeony)|
|3|SlowFast Networks for Video Recognition|[https://arxiv.org/abs/1812.03982](https://arxiv.org/abs/1812.03982)|[김주연](https://github.com/KJY477)|
|4|Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks|[https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)|[김건호](https://github.com/secripite)|
|5|A Two-stream Neural Network for Pose-based Hand Gesture Recognition|[https://arxiv.org/abs/2101.08926](https://arxiv.org/abs/2101.08926)|[최담록](https://github.com/secripite)|
</details>





